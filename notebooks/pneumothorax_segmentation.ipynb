{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO7Q/DhJ3SOCfL7h2kL3I5P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mangohehe/img-classif/blob/master/notebooks/pneumothorax_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# U-Net Model for Semantic Segmentation (with DenseNet121 Example)\n",
        "\n",
        "Paper: https://arxiv.org/abs/2009.02805\n",
        "\n",
        "This document describes the architecture and data flow of a U-Net model for semantic image segmentation, using DenseNet121 as the encoder. The model is implemented in PyTorch.\n",
        "\n",
        "## 1. Introduction\n",
        "\n",
        "Semantic segmentation is the task of assigning a class label to each pixel in an image. U-Net architectures are particularly well-suited for this because they capture both high-level contextual information and fine-grained spatial details. This implementation uses a pre-trained DenseNet121 as the encoder, leveraging transfer learning for improved performance.\n",
        "\n",
        "## 2. Model Architecture\n",
        "\n",
        "## DenseNet-U-Net Model Flowchart\n",
        "\n",
        "This flowchart visualizes the architecture and data flow of a DenseNet-U-Net model for image segmentation.\n",
        "\n",
        "    \n",
        "    A[Input: (batch_size, 3, 256, 256)] --> B{Encoder (DenseNet121)}\n",
        "\n",
        "    B --> C[Layer 0: conv0, norm0, relu0]\n",
        "    C --> D[Layer 1: pool0, denseblock1]\n",
        "    D --> E[Layer 2: transition1, denseblock2]\n",
        "    E --> F[Layer 3: transition2, denseblock3]\n",
        "    F --> G[Layer 4: transition3, denseblock4, norm5, relu5]\n",
        "\n",
        "    G --> H{Bottleneck (ConvBottleneck)}\n",
        "    H --> I[Concatenate with Upsampled Decoder Output]\n",
        "\n",
        "    I --> J{Decoder Stage 1}\n",
        "    J --> K[Upsample, Conv2d, ReLU]\n",
        "    K --> L[Concatenate with Encoder Layer 3 Output]\n",
        "\n",
        "    L --> M{Decoder Stage 2}\n",
        "    M --> N[Upsample, Conv2d, ReLU]\n",
        "    N --> O[Concatenate with Encoder Layer 2 Output]\n",
        "\n",
        "    O --> P{Decoder Stage 3}\n",
        "    P --> Q[Upsample, Conv2d, ReLU]\n",
        "    Q --> R[Concatenate with Encoder Layer 1 Output]\n",
        "\n",
        "    R --> S{Decoder Stage 4 (Last Upsample)}\n",
        "    S --> T[Upsample, Conv2d, ReLU]\n",
        "\n",
        "    T --> U[Final Layer: 1x1 Conv]\n",
        "    U --> V[Output: (batch_size, num_classes, 256, 256)]"
      ],
      "metadata": {
        "id": "kxIMi3egWbBE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrBBCS-UzRbA",
        "outputId": "684ea624-21d1-432f-c765-ea7532225488"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/kaggle/img-classif\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrcADX3j0qhB",
        "outputId": "6f455c12-7a67-4b78-f4fa-09f443fd52a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+git://github.com/albu/albumentations.git (from -r requirements.txt (line 5))\n",
            "  Cloning git://github.com/albu/albumentations.git to /tmp/pip-req-build-1nbpkyq_\n",
            "  Running command git clone --filter=blob:none --quiet git://github.com/albu/albumentations.git /tmp/pip-req-build-1nbpkyq_\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"./unet_pipeline\")"
      ],
      "metadata": {
        "id": "GkwyFDUm2zhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python Train.py experiments/albunet_valid/train_config_part0.yaml"
      ],
      "metadata": {
        "id": "QWVXmVxN25OW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}